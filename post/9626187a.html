<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="这是一个利用（可爱的）Hexo搭建的博客">
  <meta name="author" content="Edwardzcn">
  <meta name="keywords" content="Edward&#39;s blog">
  <title>机器学习——复习大纲 - Edward&#39;s blog</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->
<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">

<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">

<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Fluid</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2019-12-26 13:40">
      2019年12月26日 下午
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      3.6k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      40
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <h1 id="题型"><a href="#题型" class="headerlink" title="题型"></a>题型</h1><ul>
<li>一定会考的：决策树，神经网络，过拟合，集成学习</li>
<li>ppt 提出的问题会以选择题形式考察</li>
<li>不会考数学推导和证明</li>
</ul>
<p>必须要带计算器！</p>
<p>12.31（周二）下午 4-5 点半。B206</p>
<h1 id="Chap1-基本概念"><a href="#Chap1-基本概念" class="headerlink" title="Chap1 基本概念"></a>Chap1 基本概念</h1><p>基本术语：属性，属性值，属性空间，训练样本，测试样本</p>
<p>监督学习与非监督学习</p>
<p>区别，给出一个问题确定是哪种问题类型。分类和回归，分类和聚类。 标签的问题。</p>
<p>李：</p>
<p><strong>！！线性判别分析，两类现行判别分析，内部方差，中心距离</strong><br><strong>！！如何求判别线</strong></p>
<h1 id="Chap2-线性模型"><a href="#Chap2-线性模型" class="headerlink" title="Chap2 线性模型"></a>Chap2 线性模型</h1><h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><ul>
<li>特征尺度归一化（确保特征在相同的尺度）</li>
<li>范围归一化：使得每个特征尽量接近某个范围，如$0 \le x_i \le 1$</li>
<li>零均值归一化：用$x_i - \mu_i$替代$x_i$，即$x_i - \mu_i \rightarrow x_i$，其中$\mu_i = \frac{1}{m} \sum_{i=1}^m x_i$</li>
<li>零均值+范围归一化，如$-0.5 \le x_i \le 0.5$</li>
<li>零均值单位方差归一化：$\frac{x_i - \mu_i}{\sigma_i} \rightarrow x_i$</li>
</ul>
<h2 id="机器学习三要素"><a href="#机器学习三要素" class="headerlink" title="机器学习三要素"></a>机器学习三要素</h2><ul>
<li>模型、策略、算法？</li>
<li>数据、模型、算法？哪个对</li>
</ul>
<h2 id="线性模型基本形式"><a href="#线性模型基本形式" class="headerlink" title="线性模型基本形式"></a>线性模型基本形式</h2><script type="math/tex; mode=display">f(x) = w^{T}x+b  = \theta^T x</script><p>两种表示，注意前式中带偏秩，但是并不表达在矩阵相乘中。</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta (x^{(i)}) - y ^{(i)})^2</script><p>求解损失函数最小值，梯度下降方法为常见方法之一，梯度下降只能达到局部最优，凸函数可以达到全局最优。</p>
<ul>
<li>什么是线性关系、</li>
<li>一元线性回归，损失函数相关，最小化损失函数。</li>
<li>梯度下降法，梯度的方向。</li>
<li>欠拟合、过拟合。</li>
</ul>
<h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><ul>
<li><strong>怎样确保梯度下降算法正确的执行（收敛性）</strong><ul>
<li>自动收敛测试：每次迭代损失函数$J(\theta)$是否减少</li>
<li>收敛条件：定义不再进行迭代的收敛阈值（如$10^{-3}$）</li>
</ul>
</li>
<li><strong>怎样选择学习率$\alpha$</strong><ul>
<li>对于足够小的$\alpha$，$J(\theta)$应该在每一次迭代中减少</li>
<li>如果$\alpha$太小，梯度下降算法则会收敛很慢</li>
<li>如果$\alpha$太大，梯度下降算法则不会收敛：发散或震荡</li>
<li>用户可以以非常低的学习率开始训练模型，在每一次迭代过程中逐渐提高学习率（线性提高或者指数提高都可以），用户可以用这种方法估计出最佳学习率。</li>
</ul>
</li>
</ul>
<h2 id="改进方法"><a href="#改进方法" class="headerlink" title="改进方法"></a>改进方法</h2><ul>
<li>正规方程（The normal equations）<ul>
<li>对于求函数极小值问题，令函数微分为零，然后求解方程（而非每次对梯度递减）可得到解析解</li>
<li>西瓜数 P54 P55</li>
</ul>
</li>
</ul>
<p>现在使用的梯度下降为批量梯度下降（BGD），每次需要计算所有的样本（1-&gt;m），可以采用随机梯度下降（SGD）（随机选取一个样本，伪梯度下降）</p>
<h2 id="梯度下降的改进方法"><a href="#梯度下降的改进方法" class="headerlink" title="梯度下降的改进方法"></a>梯度下降的改进方法</h2><h3 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h3><p>动量法是一种非常简单的改进方法，已经成功应用数十年。动量法的核心思想是：在梯度方向一致的地方加速，在梯度方向不断改变的地方减速。</p>
<ul>
<li>在下降初期，使用前一次的大比重下降方向，加速。</li>
<li>在越过函数谷面时，异常的学习率，会使得两次更新方向基本相反，在原地“震荡” 此时，动量因子使得更新幅度减小，协助越过函数谷面。</li>
<li>在下降中后期，函数面局部最小值所在的吸引盆数量较多，一旦陷进吸引盆地当中，梯度趋于零，会导致止步不前，学习无法进行。如果有动量项的话，动量因子使得更新幅度增大，协助跃出吸引盆。</li>
</ul>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_19.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h3 id="NAM"><a href="#NAM" class="headerlink" title="NAM"></a>NAM</h3><p>略</p>
<a id="more"></a>
<h1 id="Chap3-logistic-回归"><a href="#Chap3-logistic-回归" class="headerlink" title="Chap3 logistic 回归"></a>Chap3 logistic 回归</h1><p>分类问题，线性模型可以通过设置阈值来达成判别。阈值选择有一点困难，因为线性模型值域 R 太广，逻辑斯特回归则希望预测函数值限制在[0,1]，$0 \le h_{\theta} (x) \le 1$。</p>
<h2 id="Sigmoid-函数的性质"><a href="#Sigmoid-函数的性质" class="headerlink" title="Sigmoid 函数的性质"></a>Sigmoid 函数的性质</h2><script type="math/tex; mode=display">g(z) = \frac{1}{1+ e^{-z}}</script><script type="math/tex; mode=display">g'(z) = g(z) (1 - g(z))</script><h2 id="概率解释"><a href="#概率解释" class="headerlink" title="概率解释"></a>概率解释</h2><script type="math/tex; mode=display">P(Y = 1 | x) = h_\theta(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^Tx}}</script><p>输入 $x$ ，输出 $y=1$ 的可能性</p>
<p>物理含义：对比线性模型的平滑过度，logistic 回归在分界值前后变化剧烈，希望达到理想的二值 Sigmoid 的函数，但由于需要合适的损失函数求解 $\theta$ 参数矩阵，没有使用不连续的二值 Sigmoid 函数</p>
<h2 id="损失函数选择"><a href="#损失函数选择" class="headerlink" title="损失函数选择"></a>损失函数选择</h2><p>回顾凸函数，线性模型带入$h_\theta(x) = \theta^Tx$入平方损失函数$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta (x^{(i)}) - y ^{(i)})^2$，得到的是凸函数，也可解。</p>
<ul>
<li>凸函数<ul>
<li>等价于 $f’’(x) \ge 0 , \forall x$</li>
<li>若$x$ 为矢量，则对应的条件变为 Hessian 矩阵$H$为半正定矩阵</li>
</ul>
</li>
<li>严格凸（小于）<ul>
<li>对于矢量，则对应条件变为 Hessian 矩阵$H$正定</li>
</ul>
</li>
</ul>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_1.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p>可以通过极大似然估计$\theta$，见西瓜书 P59</p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_2.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_3.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<ul>
<li>交叉熵损失</li>
</ul>
<p>为什么选择交叉熵而不选择平方损失，平方损失求导中间多两项，在明确分类的两端下降率接近 0，</p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_4.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_5.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_6.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h1 id="Chap4-过拟合和正则化"><a href="#Chap4-过拟合和正则化" class="headerlink" title="Chap4 过拟合和正则化"></a>Chap4 过拟合和正则化</h1><h2 id="过拟合举例"><a href="#过拟合举例" class="headerlink" title="过拟合举例"></a>过拟合举例</h2><p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_7.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_8.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h2 id="为什么出现过拟合"><a href="#为什么出现过拟合" class="headerlink" title="为什么出现过拟合"></a>为什么出现过拟合</h2><ul>
<li>最小化训练集上的损失（损失错误）</li>
<li>一般而言，模型越复杂（阶数高或特征多），训练得到的模型经验错误越低，但却更容易出现过拟合</li>
<li>选择哪个模型更合适？随机分成两部分：训练集和验证集（Validation Set）</li>
<li>训练误差和验证误差</li>
</ul>
<h2 id="如何判断是否出现了过拟合或者欠拟合"><a href="#如何判断是否出现了过拟合或者欠拟合" class="headerlink" title="如何判断是否出现了过拟合或者欠拟合"></a>如何判断是否出现了过拟合或者欠拟合</h2><p>根据曲线判断，bias（偏差）和 variance（方差），会有 trade off（分界分离），选择靠近该分界点的模型，从而减小整体误差。</p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_9.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h2 id="如何解决过拟合或者欠拟合问题"><a href="#如何解决过拟合或者欠拟合问题" class="headerlink" title="如何解决过拟合或者欠拟合问题"></a>如何解决过拟合或者欠拟合问题</h2><ul>
<li>欠拟合（Large Bias）：增加模型的附加都<ul>
<li>收集新的特征</li>
<li>增加多项式组合特征</li>
</ul>
</li>
<li>过拟合（Large Variance）<ul>
<li>增加数据（有效但是实践意义不大）</li>
<li>降低模型的复杂度<ul>
<li>减小特征：特征选择</li>
<li>正则化（Regularization）：增加偏差</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><ul>
<li>$\lambda$：正则化参数</li>
<li>问题：思考正则化参数的取值范围</li>
</ul>
<script type="math/tex; mode=display">\lambda \sum_{j=1}^{n} \theta_j^2</script><h2 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h2><p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_10.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_11.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_12.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h2 id="正则化-Logistic-回归"><a href="#正则化-Logistic-回归" class="headerlink" title="正则化 Logistic 回归"></a>正则化 Logistic 回归</h2><p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_13.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_14.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h2 id="模型性能评估"><a href="#模型性能评估" class="headerlink" title="模型性能评估"></a>模型性能评估</h2><ul>
<li>我们用训练集优化参数$\theta^{*} = arg min \frac{1}{m} \sum_{i=1}^{m} l(h_{\theta}(x^{(i)}),y^{(i)})$</li>
<li>用验证集选择模型</li>
<li>但真正关心的是模型在新的测试数据的性能（泛化能力）</li>
<li>训练集：训练参数</li>
<li>验证集（开发集，Development set）：用于调参（正规化参数、多项式阶数等）、特征选择等</li>
<li>测试集：仅仅用于性能评估</li>
</ul>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_15.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_16.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h2 id="Chap5-神经网络"><a href="#Chap5-神经网络" class="headerlink" title="Chap5 神经网络"></a>Chap5 神经网络</h2><h2 id="引入原因与神经结构"><a href="#引入原因与神经结构" class="headerlink" title="引入原因与神经结构"></a>引入原因与神经结构</h2><ul>
<li>类比人类的学习方式，The “one learning algorithm” hypotheis。人类的学习行为都是通过神经元结构完成。</li>
<li>神经元模型：Logistic unit</li>
</ul>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_17.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_18.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_19.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p>梁:</p>
<ul>
<li><strong>为什么要引入神经网络</strong></li>
<li><strong>神经网络的结构</strong></li>
<li><strong>会算前向传播和 BP</strong></li>
<li><strong>多分类，softmax，交叉熵</strong></li>
<li><strong>掌握标准激活函数，特点</strong></li>
<li><strong>掌握各种方法更新梯度</strong></li>
<li><strong>为什么不能用 0 初始化权重矩阵</strong></li>
</ul>
<p>李：</p>
<p><strong>神经网络，损失函数，表达式！</strong></p>
<h1 id="Chap6-SVM"><a href="#Chap6-SVM" class="headerlink" title="Chap6 SVM"></a>Chap6 SVM</h1><h2 id="Margin"><a href="#Margin" class="headerlink" title="Margin"></a>Margin</h2><ul>
<li>对比 Logistic 回归：在测试新样本时，当$\theta^Tx \gg 0$，或者$\ll 0$，我们可以非常自信地给出预测</li>
<li>按实际训练中我们不能很好的找到一个参数矩阵使得$\theta^Tx \gg 0$，当$y^{i} = 1$时以及相反时候。</li>
<li><p>重新定义符号</p>
</li>
<li><p>（不要求掌握优化推导和 SMO）</p>
</li>
<li>什么是支持向量，margin，分类面</li>
<li>给数据，算方程、画超平面</li>
<li>kernel，可能会有大题</li>
<li>基本概念：<ul>
<li>主问题，对偶问题，惩罚因子（分别对应对应 hard/soft SVM 去掌握）</li>
</ul>
</li>
<li>调节 kernel 参数能够防止 SVM 出现过拟合嘛？</li>
</ul>
<p>李:</p>
<ul>
<li>支持向量、怎样求最优超平面</li>
<li>核函数相关。</li>
</ul>
<h1 id="Chap6-5-决策树"><a href="#Chap6-5-决策树" class="headerlink" title="Chap6.5 决策树"></a>Chap6.5 决策树</h1><h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><ul>
<li>Shannon 1948 年提出的信息论理论</li>
<li>熵：信息量大小的度量，即表示随机变量不确定性的度量</li>
<li>事件$a_i$的信息量$I(a_i)$可如下度量：$I(a_i) = -p(a_i)\log p(a_i)$</li>
<li>假设$n$个两两不相容事件$a_1,a_2, \cdots ,a_n$，它们中有且仅有一个发生，则平均的信息量（熵）可以如下度量： $I(a_1, \cdots , a_n) = \sum_i I(a_i) = -\sum_i p(a_i) \log p(a_i)$</li>
<li>假设当前样本集 D 中第$k$类样本的比例为$p_k$，对应的信息熵为</li>
</ul>
<script type="math/tex; mode=display">Ent(D) = - \sum_k p_k \log p_k</script><ul>
<li>$Ent(D)$<strong>越小</strong>。表示数据越有序，纯度越高（一类是 0），分类效果越好</li>
<li>假设某离散属性$a$有$V$个可能值，若采用该属性对样本集划分，则会产生 V 个分支，每个分支节点包含的数据记为$D^v$</li>
<li>用属性$a$对样本集$D$进行划分，获得的信息增益为：</li>
</ul>
<script type="math/tex; mode=display">Gain(D,a) = Ent(D) - \sum_v \frac{|D^v|}{D} Ent(D^v)</script><ul>
<li>选择具有最大信息增益的属性来划分： $a^* = arg \quad max_aGain(D,a)$ （ID3 算法）</li>
</ul>
<h2 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h2><ul>
<li>ID3 算法以信息增益为选择属性，对于取值较多的属性有所偏好（带编号的显然不适合）</li>
<li>信息增益比</li>
</ul>
<script type="math/tex; mode=display">Gain\_ratio(D,a) =  \frac{Gain(D,a) }{ IV(a)}</script><p>其中 IV 成为属性的固有值（intrinsic value），属性可取值越多，IV 通常越大。</p>
<script type="math/tex; mode=display">IV(a) = - \sum_{v=1}^V \frac{|D^v|}{|D|} \log \frac{ |D^v| }{|D|}</script><ul>
<li>增益率准则对可取值数目较少的属性有所偏好</li>
<li>C4.5 并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式算法，先从划分属性中找出<strong>信息增益</strong>高于平均水平的属性，再从中选择<strong>信息增益率</strong>最高的属性 a。</li>
</ul>
<h2 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h2><ul>
<li>数据集的纯度用基尼值来衡量</li>
</ul>
<script type="math/tex; mode=display">Gini(D) = \sum_{k=1}^{|y|} \sum_{k' \not ={k}} p_k p_k' = 1 - \sum_{k=1}^{|y|} p_k^2</script><ul>
<li>直观来说，基尼值反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率，因此基尼值越小，数据集纯度越高</li>
<li>属性 a 的基尼指数定义为</li>
</ul>
<script type="math/tex; mode=display">Gini\_index(D,a) = \sum_{v=1}^{V} \frac{D^v}{D}Gini(D^v)</script><ul>
<li>CART vs. ID3<ul>
<li>二元划分：二叉树不易产生数据碎片，精确度往往也会高于多叉树</li>
<li>属性选择不同</li>
</ul>
</li>
</ul>
<h2 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h2><p>决策树的剪枝减少决策树的规模（模型复杂度），是处理“过拟合”的主要手段</p>
<h3 id="前剪枝"><a href="#前剪枝" class="headerlink" title="前剪枝"></a>前剪枝</h3><p>见西瓜书 P81 给出的方法，将数据集划分为训练集、测试集，每一次选出最优划分属性后，对测试集做划分前后比对，划分后精度下降的禁止划分（图 4.5 图 4.6 以及下面的文字表述）。</p>
<p>预剪枝基于贪心禁止分支展开，带来一定欠拟合的风险。</p>
<h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>生成完整树后做替换检查，见 P83</p>
<h2 id="连续与缺失值处理"><a href="#连续与缺失值处理" class="headerlink" title="连续与缺失值处理"></a>连续与缺失值处理</h2><h3 id="连续属性处理"><a href="#连续属性处理" class="headerlink" title="连续属性处理"></a>连续属性处理</h3><ul>
<li>离散化，最简单的是二分法</li>
<li>本质是连续的，但是对于有限采样数据是离散的</li>
<li>对于离散区间中取任意值产生的划分结果相同</li>
<li>对于包含$n-1$个元素的候选划分点集合</li>
</ul>
<script type="math/tex; mode=display">T_a = \{ \frac{a^i + a^{i+1}}{2}  |  1 \le i \le n-1 \}</script><ul>
<li>即把区间$[a^i,a^{i+1}]$的中位点作为候选划分点</li>
<li>处理时对多属性需要多一轮枚举循环，找到划分后纯度更高的划分点</li>
<li>CART 则将这些划分点当成多离散值处理，见后面。</li>
</ul>
<h3 id="CART-属性取值离散超过两种的处理"><a href="#CART-属性取值离散超过两种的处理" class="headerlink" title="CART 属性取值离散超过两种的处理"></a>CART 属性取值离散超过两种的处理</h3><ul>
<li>组合的方式转化成多个二取值问题（类似 OvR），取其中划分后 Gini_index，最小的二分情况</li>
</ul>
<h2 id="Chap7-集成学习"><a href="#Chap7-集成学习" class="headerlink" title="Chap7 集成学习"></a>Chap7 集成学习</h2><ul>
<li>构建多个学习器一起来结合来完成具体地学习任务</li>
<li>也成为多分类器系统</li>
<li>学习器可以是同类型地，也可以是不同类型</li>
<li>通过将多个学习器进行结合，常可获得比单一学习器显著优越地泛化性能，对“弱学习器”尤为明显</li>
<li>理论分析指出：假设<strong>各分类器地错误率相互独立</strong>，则随着集成个体分类器数目增大，集成的错误率将指数级下降</li>
<li>现实中个体学习器是为解决同一个问题训练出来的，不可能相互独立</li>
<li>如何产生并结合“好而不同”的个体学习器是集成学习研究的核心。</li>
</ul>
<h2 id="集成学习分类"><a href="#集成学习分类" class="headerlink" title="集成学习分类"></a>集成学习分类</h2><ul>
<li>个体学习器间存在强依赖关系，必须串行生成的序列化方法。代表：Boosting（AdaBoost，Gradient Boosting Machine）</li>
<li>个体学习器间不存在强依赖关系，可同时生成的并行化方法。代表：Bagging 和随机森林（Random Forest）</li>
</ul>
<h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><p><strong>李重点</strong></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_21.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_22.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>略</p>
<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_23.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_24.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h2 id="随机森林（Random-Forest）"><a href="#随机森林（Random-Forest）" class="headerlink" title="随机森林（Random Forest）"></a>随机森林（Random Forest）</h2><p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/MachineLearning/Review/Review_25.jpg" srcset="/img/loading.gif" alt="jpg"></p>
<h2 id="决策融合策略"><a href="#决策融合策略" class="headerlink" title="决策融合策略"></a>决策融合策略</h2><ul>
<li>平均法</li>
<li>加权平均法</li>
<li>投票法<ul>
<li>绝对大多数投票（Majority Voting）：超过半数，则决策，否则拒绝</li>
<li>少数服从多数（Plurality Voting）：得票最多的标记</li>
</ul>
</li>
<li>学习法<ul>
<li>用各学习器的输出生成新的训练数据，再去训练一个学习器（如线性 SVM 等）</li>
</ul>
</li>
</ul>
<h1 id="Chap8-聚类"><a href="#Chap8-聚类" class="headerlink" title="Chap8 聚类"></a>Chap8 聚类</h1><p>梁：</p>
<ul>
<li>GMM 不考</li>
<li>k-means 一定考</li>
<li>高斯分布和贝叶斯系列（10’）</li>
</ul>
<p>李：</p>
<ul>
<li>K-means</li>
<li>以及初始类簇中心点的确认</li>
</ul>
<h1 id="Chap9-降维"><a href="#Chap9-降维" class="headerlink" title="Chap9 降维"></a>Chap9 降维</h1><ul>
<li>（流形学习不考）</li>
<li>PCA/LDA 考其一</li>
</ul>
<h1 id="Chap10-应用"><a href="#Chap10-应用" class="headerlink" title="Chap10 应用"></a>Chap10 应用</h1><ul>
<li>各种评价指标：精度等，一级二级指标</li>
<li>混淆矩阵</li>
<li>ROC 曲线。Kappa 系数，AUC 值</li>
</ul>
<h2 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h2><p>多分类学习的拆解：OvO，OvR，MvM。投票机制，拆解原则。 海明距离、欧式距离。</p>
<p>混淆矩阵，一些指标的计算，</p>
<p>欠拟合的改良方法</p>
<h2 id="贝叶斯"><a href="#贝叶斯" class="headerlink" title="贝叶斯"></a>贝叶斯</h2><p>回顾概率公式，全概率公式，先验概率、后验概率</p>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Computer-Science/">Computer Science</a>
                    
                      <a class="hover-with-bg" href="/categories/Computer-Science/机器学习/">机器学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tag/机器学习/">机器学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/post/e9133a78.html">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">《文化商人——21世纪的出版业》读后感</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/post/68340cec.html">
                        <span class="hidden-mobile">Linux操作系统——Linux复习</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "机器学习——复习大纲&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




















</body>
</html>
