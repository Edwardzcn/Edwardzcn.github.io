

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;dark&#34;>



<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/Blog/favicon.jpg">
  <link rel="icon" type="image/png" href="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/Blog/favicon.jpg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="这是一个利用（可爱的）Hexo搭建的博客">
  <meta name="author" content="Edwardzcn">
  <meta name="keywords" content="Edward&#39;s blog">
  <title>OSDI20——Fast-RDMA-based-Ordered-Key-Value-Store-using-Remote-Learned-Cache - Edward&#39;s blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="/css/iconfont.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"www.edwardzcn98yx.com","root":"/","version":"1.8.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"onlypost":false},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/rss2.xml" title="Edward's blog" type="application/rss+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Edwardzcn</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-bookmark"></i>
                精选
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/Computer-Science/">
                    <i class="iconfont icon-codeforces"></i>
                    Computer Science
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/Life/%E9%98%85%E8%AF%BB/">
                    <i class="iconfont icon-bookmark-fill"></i>
                    Life Reading
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/Life/%E9%9A%8F%E7%AC%94/">
                    <i class="iconfont icon-note"></i>
                    Life Essay
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/Research/Report/">
                    <i class="iconfont icon-clipcheck"></i>
                    Weekly Report
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" target="_blank" rel="noopener" href="https://www.instagram.com/edwardzcn/">
                <i class="iconfont icon-instagram"></i>
                摄影集
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://eddyblog.oss-cn-shenzhen.aliyuncs.com/Blog/bg2.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="OSDI20——Fast-RDMA-based-Ordered-Key-Value-Store-using-Remote-Learned-Cache">
              
            </span>

            
              <div class="mt-3">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-author" aria-hidden="true"></i>
      Edwardzcn
    </span>
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-12-14 09:15" pubdate>
        2020年12月14日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      3.5k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      61
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">OSDI20——Fast-RDMA-based-Ordered-Key-Value-Store-using-Remote-Learned-Cache</h1>
            
            <div class="markdown-body">
              <!--文章由 3step 模板生成三步法阅读文献笔记记录-->
<h1 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h1><h2 id="题目摘要引言"><a href="#题目摘要引言" class="headerlink" title="题目摘要引言"></a>题目摘要引言</h2><h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>Fast RDMA-based Ordered Key-Value Store usng Remote Learned</p>
<p>Related work: RDMA-based ordered Key-Value store like DrTM-Tree,Cell, eRPC+Masstree</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>RDMA has gained considerable interests in <strong>network-attached in-memory</strong> key-value stores.</p>
<p>Q1: Why RDMA?</p>
<p>Challenge: traversing remote tree-based index <strong>in ordered stores</strong> ordered? oredered.</p>
<p>multiple roundtrips -&gt; order-of-magnitude slowdown | limited scalability</p>
<p>Q1: Why need multiple roundtrips.</p>
<p>As for ML</p>
<p>perfect <strong>cache structured</strong> for the tree-based index.</p>
<p>Implementation: XSTORE</p>
<p>Key idea: decouple</p>
<p>Evaluation</p>
<p>compare to DrTM-Tree, Cell, eRPC+Masstree</p>
<p>Recent effort(way):</p>
<p>propose index caching to reduce RDMA operations (doesn’t work woell with the tree-based cache)</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In-memory key-value stores with network -&gt; foundation of many datacenter applications. (database, dfs, web-services, severless computing)</p>
<p>A1: CPU becomes bottleneck and limits the scalability of increase of clients. RDMA bypass CPU. Then as for in-memory KV stores, thinking about RDMA-based KV, enable direct access to the momory of remote machines with low latency</p>
<p>Inspired by recent research using ML models an alternative index structure.</p>
<p>As for ML. Core idea.</p>
<blockquote>
<p>The client uses learned cache to predict a small range of positions for a lookup key and then fetches them using one RDMA READ. After that, the client uses a local search (e.g., scanning) to find the actual position and fetches the value using another RDMA READ.</p>
</blockquote>
<p>Challenge: using ML models. slow(frequently retraining ML models) and costly(keeping data in order) for dynamic workloads.</p>
<p>Solution: hybrid architecture, dynamic workloads(insert) -&gt; tree-based index; static workloads(gets and scans)</p>
<p>-&gt;</p>
<p>a layer of indirection</p>
<p>Q3: why need a layer of indirection to translate</p>
<h3 id="Conclusion-in-summary"><a href="#Conclusion-in-summary" class="headerlink" title="Conclusion in summary"></a>Conclusion in summary</h3><ul>
<li>learned cache</li>
<li>hybrid architecture</li>
<li><p>layer of indirection</p>
</li>
<li><p>prototype named XSTORE</p>
</li>
</ul>
<h2 id="基本理论概况"><a href="#基本理论概况" class="headerlink" title="基本理论概况"></a>基本理论概况</h2><h2 id="结论部分"><a href="#结论部分" class="headerlink" title="结论部分"></a>结论部分</h2><!-- 确定作者的成过以及点出的经验、问题、改进方 -->
<h2 id="回答基本问题"><a href="#回答基本问题" class="headerlink" title="回答基本问题"></a>回答基本问题</h2><ol>
<li><p>类别</p>
<!-- 这篇论文是什么类型？测试类？对现有系统进行分析的？对原型系统进行描述的？-->
<p>对原型系统进行描述 XSTORE</p>
</li>
<li><p>内容</p>
<!-- 你读过的其他论文有没有与之相关的？相关性与区别最直接体现在哪里？文章中分析问题用的什么理论基础（通过瞥一眼数学概况与核心证明可以得到）。 -->
<p>RDMA 相关，以前未曾涉及</p>
</li>
<li><p>正确性</p>
<!-- 结论是否看起来真实有效？ -->
<p>有效</p>
</li>
<li><p>创新点</p>
<!-- 论文的主要创新点是什么？总结提到的经验、问题和改进方法是什么？ -->
<p>使用 ML 来建模 Cache，</p>
</li>
<li><p>清晰度</p>
<!-- 这篇论文是否写的条理清晰 -->
<p>比较清晰</p>
</li>
</ol>
<h2 id="阅读选择"><a href="#阅读选择" class="headerlink" title="阅读选择"></a>阅读选择</h2><p>继续阅读</p>
<!--
1. 继续阅读：论文有价值担保，研究领域非常切合，对当前进行工作有极大帮助，圈内爆火文章
2. 不继续阅读：论文不吸引自己，论文价值/正确性存疑，论文作者给出了不适当的假设。
3. 库存保留并标记：当前不处于自己的研究领域，但不久的日后相关论文会有用。
-->
<a id="more"></a>
<h1 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h1><h2 id="细读笔记"><a href="#细读笔记" class="headerlink" title="细读笔记"></a>细读笔记</h2><!-- 要仔细阅读论文，但是诸如证明等信息要忽略掉 -->
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/nullzx/p/8729425.html">B 树与 B+树数据结构介绍</a></p>
<h3 id="RDMA-based-Key-Value-Store"><a href="#RDMA-based-Key-Value-Store" class="headerlink" title="RDMA-based Key-Value Store"></a>RDMA-based Key-Value Store</h3><p>KV: in-memory key-value store</p>
<p>network-attached: client-server model</p>
<p>tree-backed: range index structures</p>
<p>KV interfaces including $GET(K)$, $UPDATE(K,V)$, $SCAN(K,V)$, $INSERT(K,V)$ and $DELETE(K**$</p>
<p>architecture design</p>
<ul>
<li><p><strong>Server-centric design</strong>(S-RKV):</p>
<ul>
<li>Reimplement the communication layer (e.g. RPC) using RDMA primitives.</li>
<li>This design allows access to the server-side store with only two RDMA operations no matter how complex the index structures are.</li>
</ul>
</li>
<li><p><strong>Client-direct design</strong>(C-RKV):</p>
<ul>
<li>Bypass server cpu</li>
<li>May consume extra network round trips for traversing th tree-based index.</li>
<li>This commomn choice is also motivated by the read-dominated nature of most applications.</li>
<li><strong>index cache</strong>: cache top levels(due to the large tree-based index) to reduce network round trips.</li>
</ul>
</li>
</ul>
<h3 id="Analysis-and-Motivations"><a href="#Analysis-and-Motivations" class="headerlink" title="Analysis and Motivations"></a>Analysis and Motivations</h3><p><img src="https://eddyblog.oss-cn-shenzhen.aliyuncs.com/OSDI20/XSTORE_01.png" srcset="/img/loading.gif" alt="XSTORE"></p>
<ul>
<li><strong>CPU is the primary scalability bottleneck in the sever-centric design</strong><ul>
<li>Fig.2a shows that the S-RKV design makes the server rapidly saturates all CPUs</li>
<li>S-RKV just consumes 11% of network bandwith and CPU first becomes the performance bottleneck</li>
</ul>
</li>
<li><strong>Costly RDMA-based traversal is the key obstacle in the client-direct design</strong><ul>
<li>Fig. 2c sgiws RDMA-based traversal limits the peak throughput of C-RKV to 7 million requests per second, even much lower than that of S-RKV.</li>
<li>4 level cache (down from 8) Cell peaks at 14.5M reqs/s</li>
</ul>
</li>
<li><strong>Tree is not a proper structure for RDMA-based index cache</strong><ul>
<li>index is large. 理论最好情况（全部 cache）能达到 78M reqs/s，是 S-RKV 最好情况 24M reqs/s 的 3.3 倍。</li>
<li>memory-intensive but low-compute operation</li>
<li>updates to the tree-based index might progagate the changes from the leaf level to the root node, sothat the index updates would probably invalidate the cache <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3299869.3300081">recursively</a><ul>
<li><blockquote>
<p>An interesting dimension in the NAM architecture is caching of hot index nodes in compute servers that are frequently accessed. Caching allows compute servers to avoid remote memory transfers from memory servers. This is similar to the co-location of compute and memory servers.</p>
</blockquote>
</li>
<li><blockquote>
<p>For tree-based indexes, where inserts and deletes might propagate up to the index from the leaf level to the root node, we observed that cache invalidation is even a more severe issue since one insert/delete operation might need to invalidate multiple cached index nodes.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Approach-and-Overview"><a href="#Approach-and-Overview" class="headerlink" title="Approach and Overview"></a>Approach and Overview</h3><p>Using ML Models: Inspired by <a target="_blank" rel="noopener" href="https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2018_2019/papers/Kraska_SIGMOD_2018.pdf">learned index</a></p>
<p>Cumulative distribution function(CDF): map the sorted keys to the <strong>actual position</strong>, $P = CDF(K)$</p>
<p>Models: Approximate the shape of a CDF using ML models, like NN, LR. Given a lookup key (K), the model (the black curve) can predict a position (pos) with a min- and max-error, and a local search (e.g., scanning) around the prediction is used to get the actual position.</p>
<p>From the given key to a sorted array.</p>
<ul>
<li><p><strong>Learned Cache</strong>: The key idea behind XSTORE.</p>
<ul>
<li>First, cache <strong>whole</strong>(compared to <strong>partial</strong>) index at the cost of accuracy.<ul>
<li>reduce the network round trips</li>
<li>memory-efficient</li>
</ul>
</li>
<li>Second, approximately predict a range of positions for one lookup key simply (a single multiplication and addition like linear regression)<ul>
<li>also reduce end-to-end latency even compared to a whole-index cache.</li>
</ul>
</li>
<li>Finally, instead of fine-grained and recursive invalidation in the tree-based cache, ML model can delay due to approximate predictions.<ul>
<li>Save invalidation cost compared to a whole-index cache.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Challenge</strong>: Dynamic Workloads</p>
<ul>
<li>dynamic workloads (e.g., inserts and deletes) violated the “sorted” assumption</li>
<li>Model retraining is slow and costly</li>
<li>Alternative: intros delta index (server centric?) would incur more network round trips and severely increase the latency.</li>
</ul>
</li>
</ul>
<p>How to make learned cache keep pace with dynamic workloads at low cost becomes a key challenge.</p>
<ul>
<li><strong>Overview of XStore</strong>:<ul>
<li>Server hosts a B+tree index (XTREE) which store kv pairs at the leaf level physically.</li>
<li>Client use library to interact with server and host a local learned cache</li>
<li>Client-direct design for read-only requests and the server-centric design for the rest.</li>
<li>Server-centric design, transfer to remote</li>
</ul>
</li>
</ul>
<h3 id="Design-and-Implementation"><a href="#Design-and-Implementation" class="headerlink" title="Design and Implementation"></a>Design and Implementation</h3><ul>
<li><p><strong>XTree</strong>: at the server, a B+tree index(XTREE) and stores key-value pairs at the leaf level physically.</p>
<ul>
<li>optimized for remote reads in leaf node</li>
<li>24-bit incarnation</li>
<li>8-bit counter</li>
<li>32-bit right-link pointer to next sibling</li>
<li>keys with n slots and values with n slots</li>
</ul>
</li>
<li><p><strong>XCache with TT</strong>: at the client, a local learned cache (XCache) with consists of a-level recursive ML model and a translation table (TT)</p>
<ul>
<li>virtual address: Predict a range of positions (POS[..]) within a sorted array (logically stitching together all leaf nodes of XTREE)</li>
<li>TT is located by the leaf-node number (LLN) of a valid bit, a 31-bit actual leaf-node number (ALN). 24-bit incarnation and 8-bit counter.</li>
</ul>
</li>
<li><p><strong>Training models and TT</strong>:</p>
<ul>
<li>Read code</li>
<li>train each sub-model on a sorted array of its keys with a private logical position at a leaf node granularity (line 12-21) and calculate min- and max-error for every sub-model.</li>
<li>note that the keys in the leaf node across sub-models will be trained by both of sub-models (in several kset[i] due to the same predict of $xmodel.top.predict(k) \times M$).</li>
<li>Each sub-model has independent logical positions and an own translation table, making it easy to retrain a sub-model individually when necessary.</li>
</ul>
</li>
<li><p><strong>A memory-performance trade-off</strong>:</p>
<ul>
<li>model is small but TT is large</li>
<li>XMODEL with 500K sub-models only needs less than 6.7MB(14B per model)</li>
<li>100M keys, suppose each leaf node has 16 slots and is half-full, TT requires nearly 100MB. ( 100M * 64(bit) / 8(slots) /8(B/bit))</li>
<li>cache on demand</li>
</ul>
</li>
</ul>
<h3 id="Client-direct-Operations"><a href="#Client-direct-Operations" class="headerlink" title="Client-direct Operations"></a>Client-direct Operations</h3><h4 id="GET"><a href="#GET" class="headerlink" title="GET"></a>GET</h4><p>Read code eaxmple. 根据图 7 展示的 LOOKUP 流程我们知道模型预测的是一组 leaf node ID，根据 TT 表转译并通过 door bell batching 技术（该优化技术帮助我们用一次 RDMA READ 读取非连续的内存区域信息）读取到远程内存中叶子节点的信息，然后在本地进行 local search，计算所要 key 的实际地址。</p>
<p>most likely to read just one leaf node due to the low predection error of XMODEL</p>
<p>invalid in TT entry(line 9 and 17) would result in a fallback path, which ships the get operation to the server and fetches updated models and TT entries using a single request(i.e., server-centric design).</p>
<h4 id="SCAN"><a href="#SCAN" class="headerlink" title="SCAN"></a>SCAN</h4><p>find N key-value pairs (in order by key) starting with the next key at or after K.</p>
<ol>
<li>uses the lookup operation with K to determin the remote address of the first key-value pair</li>
<li>predicts the leaft nodes that contain next n key-value pairs with the help of TT(provides CNT and ALN in sorted order by key).</li>
<li>use one RDMA READ with doorbell batching to fetch these leaf nodes</li>
</ol>
<h4 id="Non-existent-Keys"><a href="#Non-existent-Keys" class="headerlink" title="Non-existent Keys"></a>Non-existent Keys</h4><p>Non-existent key 的 read，预测的键范围应该 cover 键本身。两层结构对于出于 sub-model 边界的 key 可能会产生错误预测。</p>
<p>解决办法：增加训练集，仅增加一个边界键到相邻两个 sub-models 的训练集中。</p>
<ul>
<li>由于两个子模型之间叶节点的 keys 都经过了训练，在大多数情况下不需要进行数据扩充。</li>
</ul>
<h3 id="Server-centric-Operations"><a href="#Server-centric-Operations" class="headerlink" title="Server-centric Operations"></a>Server-centric Operations</h3><ul>
<li><strong>Correctness</strong>: no lost keys</li>
<li><strong>Concurrency</strong>: HTM-based concurrent B+tree, HTM 区域+RDMA 强一致性（终止访问统一内存位置的 HTM 事务）。缓存那里不太理解。</li>
</ul>
<h4 id="UPDATE"><a href="#UPDATE" class="headerlink" title="UPDATE"></a>UPDATE</h4><p>Update to the value will not change the index, so that it will also not influence the learned cache and belongs to static worklodas.</p>
<p>Optimization: position hint. Still benefit from the learned cache.</p>
<h4 id="INSERT-and-DELETE"><a href="#INSERT-and-DELETE" class="headerlink" title="INSERT and DELETE"></a>INSERT and DELETE</h4><p>in-place inserts and deletes: XTREE chooses not to keep key-value pairs and reduces working set in the HTM region. look up based on the learned cache will not be affected since it fetches all keys</p>
<p>The oirginal leaf node should increment its incarnation, which makes the clients realize the split.</p>
<p><strong>Retraining and invalidation</strong>: The insert of a new leaf node will break the sorted(logical) order of leaf nodes and cause model retraining. Decouple model retraining from index updating until overlapped with a split.</p>
<p>Server 端后台独立进行 sub-model 的训练以及新 TT 表的构建。</p>
<p>The incorrect prediction can be detected by incarnation mismatch between the leaf node an cached TT entry (line 17 in Fig. 7) and results in a fallback, which ships the operation to the server. The client will update XCACHE with a retrained model and its translation table fetched by the fallback.</p>
<p><strong>Optimization: speculative execution</strong>: A split of leaf node just moves the second half of key-value pairs (sorted by key) to its new sibling leaf node. Therefore, the prediction to the split node must still be mapped to this node or its new sibling. 到拆分节点的映射必须拆分到该节点（移除后半部分）或者它右侧的同级节点。Based on this observation, speculative execution is enabled to handle the lookup operation on a stale TT entry (i.e., incarnation check is failed). The client will still find the look up key in the keys fetched from the split leaf node. If not found, the client will use its right-link pointer to fetch (the second half) keys from its sibling (one more RDMA READ). It means there is roughly half of the chance to avoid incurring a performance penalty. Currently only consider one sibling before using a fallback. This optimization is important for insert-dominate workloads (e.g., YCSB D) since insert operations and retraining tasks might keep server CPUs busy; the fallbacks will also take server CPU time.</p>
<p><strong>Model expansion</strong>: increase the number of sub-models in XMODEL at once (e.g., doubling) when necessary</p>
<h3 id="Durability"><a href="#Durability" class="headerlink" title="Durability"></a>Durability</h3><p>log writes for persistence and failure recovery</p>
<p>Reuse the existing durability mechanism in the concurrent tree-based index extended by XTREE, like version numbers.</p>
<h3 id="Scaling-out-XSTORE"><a href="#Scaling-out-XSTORE" class="headerlink" title="Scaling out XSTORE"></a>Scaling out XSTORE</h3><p>coarse-grained scheme，见文献 57</p>
<p>distribute an ordered key-value store span multiple servers (scale-out).</p>
<p>Assign key-value pairs based on a range-based partitioning function for keys.</p>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p><strong>Support variable-length keys</strong>: a limit, now supports fixed-length key and variable/fixed-length value. fat key pointers and solutions</p>
<p><strong>Data distribution</strong>: trade off among memory consumption of XCACHE, the retraining costs of XMODEL, and the performance of XSTRORE.</p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><h3 id="Set-up"><a href="#Set-up" class="headerlink" title="Set up"></a>Set up</h3><ul>
<li><p><strong>Testbed</strong>:</p>
<ul>
<li>15 client machines</li>
<li>12-core Intel Xeon CPUs,</li>
<li>128GB of RAM, and</li>
<li>2 ConnectX-4 100Gbps IB RNICs</li>
<li>RNIC is used by threads on the same socket and connected to a Mellanox 100Gbps IB Switch.</li>
</ul>
</li>
<li><p><strong>Workloads</strong>.</p>
<ul>
<li>mainly focus on YCSB<ul>
<li>A: update heavy</li>
<li>B: read mostly</li>
<li>C: read only</li>
<li>D: read latest</li>
<li>E: short ranges</li>
<li>F: read-modify-write</li>
</ul>
</li>
<li>100 million KV pairs initially (a 7-level tree-based index and a leaf level), 8-byte key and 8-byte value are used.</li>
</ul>
</li>
</ul>
<p>Answer questions</p>
<ul>
<li><strong>Compare targests</strong>.<ul>
<li>DrTM-Tree</li>
<li>eRPC+Masstree(noted as EMT)</li>
<li>Cell(not open-source)</li>
<li>RDMA-Memcached (noted as RMC)</li>
</ul>
</li>
</ul>
<h3 id="YCSB-Performance"><a href="#YCSB-Performance" class="headerlink" title="YCSB Performance"></a>YCSB Performance</h3><ul>
<li><p><strong>Read-only workload (YCSB C)</strong>:</p>
<ul>
<li>XSORE can achieve 82 million requests perscond (<strong>even a little higher than the optimal throughput</strong>)</li>
<li>only uses one RDMA READ to fetch one leaf node per lookup.</li>
<li>18% drop in Zipfian distribution. suspec that<ul>
<li>多个 clients 读取小范围内存</li>
<li>怀疑 RNIC 根据请求地址检查 RDMA 操作之间的冲突，即便没有冲突，这样的检查也会争夺 processing resources。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Static read-write workloads (YCSB A,B, and F)</strong>:</p>
<ul>
<li>still bottlenecked by server CPUs for handling updates but better than server-centric design</li>
<li>YCSB even higher YCSB C<ul>
<li>the read requests are less skewed interleaved with (10%) updates, compared to read-only workloads(YCSB C)</li>
<li>the server of XSTORE has not been saturated ; thus itis still sufficient to perform updates, compared to update-heavy workloads(YCSB A)</li>
</ul>
</li>
<li>YCSB 75% between A and B</li>
</ul>
</li>
<li><p><strong>Dynamic workloads (YCSB D and E)</strong></p>
<ul>
<li><p>contention(performance slowdown) happs differently</p>
<ul>
<li>For DrTM-Tree and EMT, on tree-based index</li>
<li>For XSORE and Cell, cache invalidations.</li>
</ul>
</li>
<li><p>overhead for XSTORE</p>
<ul>
<li>hard to learn than static so prediction error increase (dynamic workloads distribution is close to noised linear)</li>
<li>_optimal_ means a whole-index cache. 3x slower than static workload due to cache invalidations. latest distribution significantly reduces cache misses and invalidations, so is better than Uniform</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>CPU utilizations of XSTORE</strong>:</p>
<ul>
<li>no graphs</li>
<li>saves server CPUs compared to sever-centric(under half compared to 100% in DrTM-Tree)</li>
</ul>
</li>
<li><p><strong>End-to-end latency</strong>:</p>
<ul>
<li>YCSB C uniform distribution(others similar)</li>
<li>when low load, server-centric KVs have lower latency, as one RPC round trip is faster than two one-sided RDMA operations(NIC_RPC compare to NIC_IDX and NIC_VAL)</li>
</ul>
</li>
</ul>
<h3 id="Production-Workload-Performance"><a href="#Production-Workload-Performance" class="headerlink" title="Production Workload Performance"></a>Production Workload Performance</h3><p>similar to YCSB A</p>
<h3 id="Scale-out-Performance"><a href="#Scale-out-Performance" class="headerlink" title="Scale-out Performance"></a>Scale-out Performance</h3><p>Perform better with the increase of RNICs. 6 server RNICs in 3 server machines. up to 145M reqs/s which is limited by the number of client machines.</p>
<p><strong>XSTORE needs about eight client RNICs to saturate one server RNIC</strong></p>
<h3 id="Model-Re-Training-and-Expansion"><a href="#Model-Re-Training-and-Expansion" class="headerlink" title="Model (Re-)Training and Expansion"></a>Model (Re-)Training and Expansion</h3><p>训练速度需要与动态负载的 insertion 速度匹配，模型 retraining 跟不上会影响整体的 throughput。</p>
<p><em>*For dynamic workloads, the throughput of XSTORE would decrease whe stale sub-models can not retrained in time</em></p>
<p>图 14 对比说明了该问题。</p>
<p>可以设定随着 keys 数增多，达到一定阈值后，重新训练 sub-models 增加的新模型。Model Expansion</p>
<h2 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h2><h2 id="未读（且值得读）文献记录"><a href="#未读（且值得读）文献记录" class="headerlink" title="未读（且值得读）文献记录"></a>未读（且值得读）文献记录</h2><ul>
<li><p>State-of-the-art RDMA-based ordered KV Stores Design</p>
<ul>
<li><a href="">11: Fast and general distributed transactions using rdma and htm | EuroSys ‘16</a></li>
<li><a href="">24:</a></li>
<li><a href="">35:</a></li>
</ul>
</li>
<li><p>Concurrent B+ Tree</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://pdos.csail.mit.edu/papers/masstree:eurosys12.pdf">33: Cache craftiness for fast multicore key-value storage | EuroSys ‘12</a></li>
<li><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/2592798.2592815">50: Using restrictedtransactional memory to build a scalable in-memory database | EuroSys ‘14</a></li>
<li><a href="">57: Designing distributed tree-based index structures for fast rdma-capable networks</a></li>
</ul>
</li>
</ul>
<!-- 在第二步中，找到以上几篇论文中相同的引用文献、在引用文献中重复的作者名，这些就是你所调研领域的重点论文和科研人员，下载那些重点论文暂时搁到一边。继续访问那些重要科研人员的网页，看下那些论文是在什么会议集结出版的。这能帮你找到那个领域的顶级会议，因为那些重要科研人员通常会在顶级会议发表论文。 -->
<h1 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h1><!-- 试着“在脑中重新实现（virtually re-implement）”那篇论文：也就是，和作者一样做出相同的假设，然后重新实现相同的工作。通过对比你自己得出的和论文中给出的结果，不仅可以很轻易验证一篇论文的创新点是否真实，而且还能发现论文中不会讲的缺点和假想。 -->
<h2 id="思路复现"><a href="#思路复现" class="headerlink" title="思路复现"></a>思路复现</h2><!-- 推敲作者是如何立题，如何找到突破口，在其中是否有抉择，如何做出的假设？ -->
<h3 id="关于两次-RDMA-READ"><a href="#关于两次-RDMA-READ" class="headerlink" title="关于两次 RDMA READ"></a>关于两次 RDMA READ</h3><p>email 询问作者魏星达的回复</p>
<blockquote>
<p>问题名称：XTree 叶子节点的 Value 保存的是指具体的数据，还是指向数据部分的地址。</p>
<p>XSTORE 在 value 时定长的时候保存的是具体的数据，不定长的时候储存的是地址。</p>
<p>这里我疑惑如果保存的是具体数据，我们为什么还需要在这里确定 key 的“实际地址”后再进行一次 RDMA READ，value 值不是可以再叶子节点中读取到吗？</p>
<p>你的问题是正确的。理论来说，如果叶子存的是具体数据，那么一次 RDMA 就可以完全读回。这边 XSTORE 使用两次是为了节约 RDMA 读的 payload。</p>
<p>举例来说，如果使用一次 RDMA，则需要读取叶子里的所有数据加上 $Key$，这样会读取过多的值 $(n <em> sizeof(Key) + n </em> sizeof(Value)$,其中 $n$ 是叶子节点最多的 item 数量），造成 RDMA bandwidth 的浪费。</p>
<p>如果使用两次 RDMA，则总共需要读 $n * sizeof(Key) + sizeof(Value)$ 数据，如果 $Value$ 的大小比 $Key$ 大很多（通常是这种情况），则会更加高效。</p>
</blockquote>
<h2 id="证明与推理复现"><a href="#证明与推理复现" class="headerlink" title="证明与推理复现"></a>证明与推理复现</h2><!-- 分析证明过程，确定证明的正确性以及完整性。对证明过程做拓展，调研证明思路来源 -->
<h2 id="实验验证复现"><a href="#实验验证复现" class="headerlink" title="实验验证复现"></a>实验验证复现</h2><h2 id="Original-Presentation"><a href="#Original-Presentation" class="headerlink" title="Original Presentation"></a>Original Presentation</h2><h3 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h3><ol>
<li>Traditional KVS uses RPC(Server-centric) -&gt; kv will close focus on the server cpu.</li>
</ol>
<p>lead to huge cpu cost with random reads (nic’s bandwith &gt; cpu frequency)</p>
<ol>
<li>One-sided RDMA(client-direct)</li>
</ol>
<p>workload shift from the server cpu to the client cpu -&gt; client-direct</p>
<p>simple read/write -&gt; easy for hashtable but hard for tree-based index structures</p>
<p>QQ1: Why n RTT?</p>
<p>QCache:</p>
<ol>
<li>existing systems adopt caching</li>
</ol>
<p>cost a huge amount of memory</p>
<p>copare the three designs of RDMA usage</p>
<h3 id="XSTORE"><a href="#XSTORE" class="headerlink" title="XSTORE"></a>XSTORE</h3><ol>
<li>Server-centric updates</li>
<li>Because one-sided has simple semantic</li>
</ol>
<p>insert -&gt; tree split and let it to the remote CPU</p>
<h3 id="Learned-cache"><a href="#Learned-cache" class="headerlink" title="Learned cache"></a>Learned cache</h3><p>Realted: Learned index</p>
<p>索引是模型:b 树索引可以看作是一个模型，它是一个记录在排序数组中的位置的键</p>
<p>Key —index model—&gt; Address</p>
<h4 id="Client-direct-get-using-learned-cache"><a href="#Client-direct-get-using-learned-cache" class="headerlink" title="Client-direct get() using learned cache"></a>Client-direct get() using learned cache</h4><p>first train model at the server(small memory usage)</p>
<p>transfer from server to the client</p>
<p>Note: learned model assumes that the KV are stored in a sorted array</p>
<ul>
<li><h1 id="1-1-RTT-for-lookup-positions-gt-keys"><a href="#1-1-RTT-for-lookup-positions-gt-keys" class="headerlink" title="1 1 RTT for lookup (positions -&gt; keys)"></a>1 1 RTT for lookup (positions -&gt; keys)</h1></li>
<li><h1 id="2-Small-memory-footprint"><a href="#2-Small-memory-footprint" class="headerlink" title="2 Small memory footprint"></a>2 Small memory footprint</h1></li>
</ul>
<p>Challenge: Dynamic insertions/deletions? Leaned model assumes a sorted array(inefficient to support for insertions and deletions</p>
<h4 id="XSTORE-1"><a href="#XSTORE-1" class="headerlink" title="XSTORE"></a>XSTORE</h4><p>Client-&gt; model &amp; TT</p>
<h3 id="2020-12-14-meeting"><a href="#2020-12-14-meeting" class="headerlink" title="2020/12/14 meeting"></a>2020/12/14 meeting</h3><p>RDMA design</p>
<p>Cache</p>
<ol>
<li>Cache too large</li>
<li>Cache miss</li>
<li>Random acces</li>
</ol>
<p>Why 2 times RDMA -&gt; TT</p>
<p>Decouple insert O1 challenge</p>
<p>ML correct about the insertion</p>
<p>Training</p>
<h3 id="2020-12-19-meeting"><a href="#2020-12-19-meeting" class="headerlink" title="2020/12/19 meeting"></a>2020/12/19 meeting</h3><ol>
<li>B+ 树设计 Cache ，positions 预测</li>
<li>TT 表的理解<br>1.</li>
<li>CDF 曲线样式可能和</li>
</ol>
<p>自己的序号+1，为什么原来的 XCAHE 模型可以，</p>
<h2 id="新的问题"><a href="#新的问题" class="headerlink" title="新的问题"></a>新的问题</h2><ol>
<li><p>retraining，保证你本地的 TT 表能更新，能支持动态的 workloads</p>
</li>
<li><p>client 端 TT 表更新策略</p>
</li>
<li><p>HTM transaction</p>
</li>
</ol>
<p>接近 1h：18min，拓展</p>
<p>补：RDMA 上的工作，引入 Learned Cache，</p>
<h3 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h3><ul>
<li>Background</li>
<li>Motivation<ul>
<li>Analysis 3. 原</li>
<li>Learned Cache 4 Cache 模型本身设计问题，</li>
</ul>
</li>
<li><p>Contribution 部分</p>
<ul>
<li>阶段性总结</li>
</ul>
</li>
<li><p>Model</p>
<ul>
<li>Hybrid architecture</li>
</ul>
</li>
<li><p>Implementation</p>
<ul>
<li>XSTORE Cache + TT 表</li>
</ul>
</li>
<li><p>Evalutaion（图表？）</p>
</li>
<li>Conclution<ul>
<li>搬运原有文章</li>
</ul>
</li>
</ul>
<!-- Rework -->

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Research/">Research</a>
                    
                      <a class="hover-with-bg" href="/categories/Research/Paper/">Paper</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tag/Paper/">Paper</a>
                    
                      <a class="hover-with-bg" href="/tag/%E4%B8%89%E6%AD%A5%E6%B3%95/">三步法</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/post/7dddccce.html">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">OSDI20——From-WiscKey-to-Bourbon-A-Learned-Index-for-Log-Structured-Merge-Trees</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/post/62b7bfee.html">
                        <span class="hidden-mobile">Coq学习——《Software-Foundation》Volume1.9-ProofObjects</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>

<!-- SCRIPTS -->

  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      var inputArea = document.querySelector("#local-search-input");
      inputArea.onclick = function () {
        searchFunc(path, 'local-search-input', 'local-search-result');
        this.onclick = null
      }
    })()
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



</body>
</html>
